{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Types:\n",
    "\n",
    "- `zs`: An AI agent initialized with a zero-shot prompt. Just the gave description.\n",
    "- `spp`: Solo Performance Prompting; an AI agent initialized with the SPP prompt.\n",
    "- `cot`: Chain-of-Thought; an AI agent initialized with the COT prompt.\n",
    "- `srep`: Singe-Round-Equilibrium-Player; a player who strictly follows the Single Round Equilibrium Strategy (a specific probability distribution over the available moves)\n",
    "- `pp`: Pattern Player; Follows a cyclic pattern of moves. Always playes moves from this pattern.\n",
    "- `ap`: Adaptive Player; finds the most frequent move their opponent plays and counters it.\n",
    "- `tft`: Tit-for-Tat Player; counters opponent's last played move.\n",
    "\n",
    "## SC experiments:\n",
    "\n",
    "- We compare an AI agent in a SC environment vs all other agents. If the opponent is also an AI agent, then the opponent in **NOT** a SC player.\n",
    "- SC: Each time the AI agent has to play. They generate 5 different answers. We then choose the most frequent result and choose an answer that gave that result. We continue with this history for the rest of the game (as many rounds as it is). Conficts are resolved at random and/or by choosing the first answer that gave the result we picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        \"id\" : \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        \"name\" : \"Claude 3.5 Sonnet v2\",\n",
    "        \"thinking\" : False,\n",
    "    },\n",
    "    {\n",
    "        \"id\" : \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        \"name\" : \"Claude 3.7 Sonnet\",\n",
    "        \"thinking\" : False,\n",
    "    },\n",
    "    {\n",
    "        \"id\" : \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        \"name\" : \"Claude 3.7 Sonnet (Thinking)\",\n",
    "        \"thinking\" : True,\n",
    "    },\n",
    "    {\n",
    "        \"id\" : \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        \"name\" : \"Claude Sonnet 4\",\n",
    "        \"thinking\" : False,\n",
    "    },\n",
    "    {\n",
    "        \"id\" : \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        \"name\" : \"Claude Sonnet 4 (Thinking)\",\n",
    "        \"thinking\" : True,\n",
    "    },\n",
    "    {\n",
    "        \"id\" : \"us.meta.llama3-3-70b-instruct-v1:0\",\n",
    "        \"name\" : \"Llama 3.3 70B Instruct\",\n",
    "        \"thinking\" : False,\n",
    "    },\n",
    "    {\n",
    "        \"id\" : \"mistral.mistral-large-2407-v1:0\",\n",
    "        \"name\" : \"Mistral Large (24.07)\",\n",
    "        \"thinking\" : False,\n",
    "    },\n",
    "    {\n",
    "        \"id\" : \"us.deepseek.r1-v1:0\",\n",
    "        \"name\" : \"DeepSeek-R1\",\n",
    "        \"thinking\" : False,\n",
    "    },\n",
    "]\n",
    "\n",
    "game_settings_types = [\"pd\", \"pd-alt\", \"sh\", \"sh-alt\"]\n",
    "\n",
    "prompt_types = [\"zs\", \"spp\", \"cot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_total_points_dataframe(\n",
    "    log_dir: str,\n",
    "    model_names: list[str],\n",
    "    prompt_types: list[str],\n",
    "    game_type: str,\n",
    "    game_settings_type: str,\n",
    "    iteration_cnt: int,\n",
    "    tot: bool,\n",
    ") -> pd.DataFrame:\n",
    "    y_replacements = {\n",
    "        \"zs\": \"zs\",\n",
    "    } if not tot else {\n",
    "        \"zs\": \"sc-zs\",\n",
    "        \"spp\": \"sc-spp\",\n",
    "        \"cot\": \"sc-cot\",\n",
    "    }\n",
    "    x_replacements = {\n",
    "        \"zs\": \"zs\",\n",
    "    } \n",
    "\n",
    "    # {(model, prompt) -> {opponent_type -> total_points}}\n",
    "    heatmap_data = defaultdict(lambda: defaultdict(float))\n",
    "    opponent_set = set()\n",
    "\n",
    "    for model in model_names:\n",
    "        for prompt in prompt_types:\n",
    "            for itr in range(iteration_cnt):\n",
    "                directory = os.path.join(log_dir, f\"iteration_{itr}\", model, game_type, game_settings_type)\n",
    "\n",
    "                if not os.path.isdir(directory):\n",
    "                    continue\n",
    "\n",
    "                for game_dir in sorted(os.listdir(directory)):\n",
    "                    info_path = os.path.join(directory, game_dir, 'game.json')\n",
    "                    if not os.path.isfile(info_path):\n",
    "                        continue\n",
    "\n",
    "                    with open(info_path) as f:\n",
    "                        info = json.load(f)\n",
    "\n",
    "                    player_types = [info.get(f\"player_{i}_player_type\") for i in range(2)]\n",
    "                    if prompt not in player_types:\n",
    "                        continue\n",
    "\n",
    "                    model_idx = player_types.index(prompt)\n",
    "                    if model_idx != 0:\n",
    "                        continue\n",
    "\n",
    "                    opponent_type = player_types[1 - model_idx]\n",
    "                    opponent_set.add(opponent_type)\n",
    "\n",
    "                    total_points = info.get(f\"player_{model_idx}_total_points\")\n",
    "                    if total_points is None:\n",
    "                        raise ValueError(f\"Missing total_points for {info_path}\")\n",
    "\n",
    "                    heatmap_data[(model, prompt)][opponent_type] += total_points\n",
    "\n",
    "    if not heatmap_data:\n",
    "        raise ValueError(\"No data collected â€” check log paths and model+prompt naming conventions.\")\n",
    "\n",
    "    for key in heatmap_data:\n",
    "        for opponent in heatmap_data[key]:\n",
    "            heatmap_data[key][opponent] /= iteration_cnt\n",
    "\n",
    "    opponent_types_aux = [\"zs\", \"spp\", \"cot\", \"srep\", \"pp\", \"mf\", \"tft\"]\n",
    "    sorted_opponents = [opp for opp in opponent_types_aux if opp in opponent_set]\n",
    "    model_prompt_keys = [(model, prompt) for model in model_names for prompt in prompt_types]\n",
    "\n",
    "    # Apply x label replacements\n",
    "    x_labels = sorted_opponents.copy()\n",
    "    for old, new in x_replacements.items():\n",
    "        x_labels = [label.replace(old, new) for label in x_labels]\n",
    "\n",
    "    rows = []\n",
    "    index_tuples = []\n",
    "\n",
    "    for model, prompt in model_prompt_keys:\n",
    "        new_prompt = prompt\n",
    "        for old, new in y_replacements.items():\n",
    "            new_prompt = new_prompt.replace(old, new)\n",
    "\n",
    "        index_tuples.append( (model, new_prompt) )\n",
    "\n",
    "        values = []\n",
    "        for opp in sorted_opponents:\n",
    "            val = heatmap_data.get((model, prompt), {}).get(opp, -1000)\n",
    "            values.append(val)\n",
    "        rows.append(values)\n",
    "\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(index_tuples, names=[\"model\", \"prompt\"])\n",
    "    df = pd.DataFrame(rows, index=index, columns=x_labels)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_nonsc_tp = [\n",
    "    get_total_points_dataframe(\n",
    "        log_dir=\"../logs_pd/logs_3/data\",\n",
    "        model_names=[model[\"name\"] for model in models],\n",
    "        prompt_types=prompt_types,\n",
    "        game_type=\"pd\",\n",
    "        game_settings_type=game_settings_type,\n",
    "        iteration_cnt=5,\n",
    "        tot=False,\n",
    "    )\n",
    "    for game_settings_type in game_settings_types\n",
    "]\n",
    "\n",
    "dfs_sc_tp = [\n",
    "    get_total_points_dataframe(\n",
    "        log_dir=\"../logs_pd/logs_3/data_tot\",\n",
    "        model_names=[model[\"name\"] for model in models],\n",
    "        prompt_types=prompt_types,\n",
    "        game_type=\"pd\",\n",
    "        game_settings_type=game_settings_type,\n",
    "        iteration_cnt=2,\n",
    "        tot=True,\n",
    "    )\n",
    "    for game_settings_type in game_settings_types\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_order = ['zs', 'cot', 'spp', 'sc-zs', 'sc-cot', 'sc-spp']\n",
    "prompt_order_map = {prompt: i for i, prompt in enumerate(prompt_order)}\n",
    "\n",
    "dfs_merged_tp = []\n",
    "\n",
    "for df_nonsc, df_sc in zip(dfs_nonsc_tp, dfs_sc_tp):\n",
    "    df_nonsc = df_nonsc.copy()\n",
    "    df_sc = df_sc.copy()\n",
    "\n",
    "    # Prefix the prompt types in sc\n",
    "    new_index = []\n",
    "    for model, prompt in df_sc.index:\n",
    "        new_index.append((model, prompt))\n",
    "    df_sc.index = pd.MultiIndex.from_tuples(new_index, names=df_sc.index.names)\n",
    "\n",
    "    # Concatenate vertically\n",
    "    merged_df = pd.concat([df_nonsc, df_sc])\n",
    "\n",
    "    # Reorder by (model, prompt) with custom prompt order\n",
    "    merged_df = merged_df.reset_index()\n",
    "\n",
    "    # Add a sort key column\n",
    "    merged_df['prompt_order'] = merged_df['prompt'].map(prompt_order_map)\n",
    "\n",
    "    # Sort by model, then prompt order\n",
    "    merged_df = merged_df.sort_values(['model', 'prompt_order'])\n",
    "\n",
    "    # Drop helper column\n",
    "    merged_df = merged_df.drop(columns=['prompt_order'])\n",
    "\n",
    "    # Restore MultiIndex\n",
    "    merged_df = merged_df.set_index(['model', 'prompt'])\n",
    "\n",
    "    dfs_merged_tp.append(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, game_settings_type in zip(dfs_merged_tp, game_settings_types):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify maxima\n",
    "    row_max_mask = df.eq(df.max(axis=1), axis=0)\n",
    "    col_max_mask = df.eq(df.max(axis=0), axis=1)\n",
    "\n",
    "    # Make values strings, bold if max in row or column\n",
    "    styled_df = df.copy().astype(str)\n",
    "    for row in df.index:\n",
    "        for col in df.columns:\n",
    "            val = df.loc[row, col]\n",
    "            is_max = row_max_mask.loc[row, col] or col_max_mask.loc[row, col]\n",
    "            if pd.isna(val):\n",
    "                formatted = \"\"\n",
    "            else:\n",
    "                formatted = f\"\\\\textbf{{{val:.1f}}}\" if is_max else f\"{val:.1f}\"\n",
    "            styled_df.loc[row, col] = formatted\n",
    "\n",
    "    # Add game_settings_type as MultiIndex header\n",
    "    styled_df.columns = pd.MultiIndex.from_product(\n",
    "        [[game_settings_type], styled_df.columns],\n",
    "    )\n",
    "\n",
    "    # Output LaTeX\n",
    "    latex_code = styled_df.to_latex(\n",
    "        index=True,\n",
    "        multirow=True,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format='c',\n",
    "        escape=False,  # Allow \\textbf\n",
    "        caption=f\"Total Points Averaged Over All Iterations ({game_settings_type})\",\n",
    "        label=f\"tab:pd_total_points_avg_heatmap_{game_settings_type}\",\n",
    "    )\n",
    "    print(latex_code)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_efficiency_dataframe(\n",
    "    log_dir: str,\n",
    "    model_names: list[str],\n",
    "    prompt_types: list[str],\n",
    "    game_type: str,\n",
    "    game_settings_type: str,\n",
    "    iteration_cnt: int,\n",
    "    tot: bool,\n",
    ") -> pd.DataFrame:\n",
    "    y_replacements = {\n",
    "        \"zs\": \"zs\",\n",
    "    } if not tot else {\n",
    "        \"zs\": \"sc-zs\",\n",
    "        \"spp\": \"sc-spp\",\n",
    "        \"cot\": \"sc-cot\",\n",
    "    }\n",
    "    x_replacements = {\n",
    "        \"zs\": \"zs\",\n",
    "    } \n",
    "\n",
    "    # {(model, prompt) -> {opponent_type -> total_points}}\n",
    "    efficiency_data = defaultdict(lambda: defaultdict(float))\n",
    "    opponent_set = set()\n",
    "\n",
    "    for model in model_names:\n",
    "        for prompt in prompt_types:\n",
    "            for itr in range(iteration_cnt):\n",
    "                directory = os.path.join(log_dir, f\"iteration_{itr}\", model, game_type, game_settings_type)\n",
    "\n",
    "                if not os.path.isdir(directory):\n",
    "                    continue\n",
    "\n",
    "                for game_dir in sorted(os.listdir(directory)):\n",
    "                    info_path = os.path.join(directory, game_dir, 'game.json')\n",
    "                    if not os.path.isfile(info_path):\n",
    "                        continue\n",
    "\n",
    "                    with open(info_path) as f:\n",
    "                        info = json.load(f)\n",
    "\n",
    "                    player_types = [info.get(f\"player_{i}_player_type\") for i in range(2)]\n",
    "                    if prompt not in player_types:\n",
    "                        continue\n",
    "\n",
    "                    model_idx = player_types.index(prompt)\n",
    "                    if model_idx != 0:\n",
    "                        continue\n",
    "\n",
    "                    opponent_type = player_types[1 - model_idx]\n",
    "                    opponent_set.add(opponent_type)\n",
    "\n",
    "                    tokens = info.get(f\"player_{model_idx}_tokens\")\n",
    "                    total_points = info.get(f\"player_{model_idx}_total_points\")\n",
    "\n",
    "                    if tokens is None or total_points is None:\n",
    "                        print(f\"Model {model}, Prompt {prompt}, Iteration {itr}, Game directory {game_dir} - Missing data in {info_path}\")\n",
    "                        raise ValueError(f\"Missing tokens or total_points for {info_path}\")\n",
    "\n",
    "                    efficiency_data[(model, prompt)][opponent_type] += total_points / max(tokens) * 1000  # Scale to per 1000 tokens\n",
    "\n",
    "    if not efficiency_data:\n",
    "        raise ValueError(\"No data collected â€” check log paths and model+prompt naming conventions.\")\n",
    "\n",
    "    for key in efficiency_data:\n",
    "        for opponent in efficiency_data[key]:\n",
    "            efficiency_data[key][opponent] /= iteration_cnt\n",
    "\n",
    "    opponent_types_aux = [\"zs\", \"spp\", \"cot\", \"srep\", \"pp\", \"mf\", \"tft\"]\n",
    "    sorted_opponents = [opp for opp in opponent_types_aux if opp in opponent_set]\n",
    "    model_prompt_keys = [(model, prompt) for model in model_names for prompt in prompt_types]\n",
    "\n",
    "    for key in efficiency_data:\n",
    "        aux = 0\n",
    "        for opp in sorted_opponents:\n",
    "            aux += efficiency_data[key][opp]\n",
    "        efficiency_data[key][\"avg\"] = aux / len(sorted_opponents)\n",
    "\n",
    "    sorted_opponents.append(\"avg\")  # Add average to the end of the list\n",
    "\n",
    "    # Apply x label replacements\n",
    "    x_labels = sorted_opponents.copy()\n",
    "    for old, new in x_replacements.items():\n",
    "        x_labels = [label.replace(old, new) for label in x_labels]\n",
    "\n",
    "    rows = []\n",
    "    index_tuples = []\n",
    "\n",
    "    for model, prompt in model_prompt_keys:\n",
    "        new_prompt = prompt\n",
    "        for old, new in y_replacements.items():\n",
    "            new_prompt = new_prompt.replace(old, new)\n",
    "\n",
    "        index_tuples.append( (model, new_prompt) )\n",
    "\n",
    "        values = []\n",
    "        for opp in sorted_opponents:\n",
    "            val = efficiency_data.get((model, prompt), {}).get(opp, -1000)\n",
    "            values.append(val)\n",
    "        rows.append(values)\n",
    "\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(index_tuples, names=[\"model\", \"prompt\"])\n",
    "    df = pd.DataFrame(rows, index=index, columns=x_labels)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_nonsc_ef = [\n",
    "    get_efficiency_dataframe(\n",
    "        log_dir=\"../logs_pd/logs_3/data\",\n",
    "        model_names=[model[\"name\"] for model in models],\n",
    "        prompt_types=prompt_types,\n",
    "        game_type=\"pd\",\n",
    "        game_settings_type=game_settings_type,\n",
    "        iteration_cnt=5,\n",
    "        tot=False,\n",
    "    )\n",
    "    for game_settings_type in game_settings_types\n",
    "]\n",
    "\n",
    "dfs_sc_ef = [\n",
    "    get_efficiency_dataframe(\n",
    "        log_dir=\"../logs_pd/logs_3/data_tot\",\n",
    "        model_names=[model[\"name\"] for model in models],\n",
    "        prompt_types=prompt_types,\n",
    "        game_type=\"pd\",\n",
    "        game_settings_type=game_settings_type,\n",
    "        iteration_cnt=2,\n",
    "        tot=True,\n",
    "    )\n",
    "    for game_settings_type in game_settings_types\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_order = ['zs', 'cot', 'spp', 'sc-zs', 'sc-cot', 'sc-spp']\n",
    "prompt_order_map = {prompt: i for i, prompt in enumerate(prompt_order)}\n",
    "\n",
    "dfs_merged_ef = []\n",
    "\n",
    "for df_nonsc, df_sc in zip(dfs_nonsc_ef, dfs_sc_ef):\n",
    "    df_nonsc = df_nonsc.copy()\n",
    "    df_sc = df_sc.copy()\n",
    "\n",
    "    # Prefix the prompt types in sc\n",
    "    new_index = []\n",
    "    for model, prompt in df_sc.index:\n",
    "        new_index.append((model, prompt))\n",
    "    df_sc.index = pd.MultiIndex.from_tuples(new_index, names=df_sc.index.names)\n",
    "\n",
    "    # Concatenate vertically\n",
    "    merged_df = pd.concat([df_nonsc, df_sc])\n",
    "\n",
    "    # Reorder by (model, prompt) with custom prompt order\n",
    "    merged_df = merged_df.reset_index()\n",
    "\n",
    "    # Add a sort key column\n",
    "    merged_df['prompt_order'] = merged_df['prompt'].map(prompt_order_map)\n",
    "\n",
    "    # Sort by model, then prompt order\n",
    "    merged_df = merged_df.sort_values(['model', 'prompt_order'])\n",
    "\n",
    "    # Drop helper column\n",
    "    merged_df = merged_df.drop(columns=['prompt_order'])\n",
    "\n",
    "    # Restore MultiIndex\n",
    "    merged_df = merged_df.set_index(['model', 'prompt'])\n",
    "\n",
    "    dfs_merged_ef.append(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, game_settings_type in zip(dfs_merged_ef, game_settings_types):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify maxima\n",
    "    row_max_mask = df.eq(df.max(axis=1), axis=0)\n",
    "    col_max_mask = df.eq(df.max(axis=0), axis=1)\n",
    "\n",
    "    # Make values strings, bold if max in row or column\n",
    "    styled_df = df.copy().astype(str)\n",
    "    for row in df.index:\n",
    "        for col in df.columns:\n",
    "            val = df.loc[row, col]\n",
    "            is_max = row_max_mask.loc[row, col] or col_max_mask.loc[row, col]\n",
    "            if pd.isna(val):\n",
    "                formatted = \"\"\n",
    "            else:\n",
    "                formatted = f\"\\\\textbf{{{val:.2f}}}\" if is_max else f\"{val:.2f}\"\n",
    "            styled_df.loc[row, col] = formatted\n",
    "\n",
    "    # Add game_settings_type as MultiIndex header\n",
    "    styled_df.columns = pd.MultiIndex.from_product(\n",
    "        [[game_settings_type], styled_df.columns],\n",
    "    )\n",
    "\n",
    "    # Output LaTeX\n",
    "    latex_code = styled_df.to_latex(\n",
    "        index=True,\n",
    "        multirow=True,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format='c',\n",
    "        escape=False,  # Allow \\textbf\n",
    "        caption=f\"Average Efficiency (Points per kilo-token) ({game_settings_type})\",\n",
    "        label=f\"tab:rps_efficiency_avg_heatmap_{game_settings_type}\",\n",
    "    )\n",
    "    print(latex_code)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_move(move1, move2, game_settings):\n",
    "    # is move1 the optimal move against move2?\n",
    "    if move2 == game_settings[\"a\"]:\n",
    "        points = game_settings[\"aa\"] if move1 == game_settings[\"a\"] else game_settings[\"ba\"]\n",
    "        op_points = game_settings[\"aa\"] if move2 == game_settings[\"a\"] else game_settings[\"ab\"]\n",
    "        return points >= op_points\n",
    "    elif move2 == game_settings[\"b\"]:\n",
    "        points = game_settings[\"ab\"] if move1 == game_settings[\"a\"] else game_settings[\"bb\"]\n",
    "        op_points = game_settings[\"ba\"] if move2 == game_settings[\"a\"] else game_settings[\"bb\"]\n",
    "        return points >= op_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_round_of_understood_opponent(\n",
    "    log_dir: str,\n",
    "    model_names: list[str],\n",
    "    prompt_types: list[str],\n",
    "    game_type: str,\n",
    "    game_settings_type: str,\n",
    "    iter_cnt: int,\n",
    "    tot: bool,\n",
    ") -> pd.DataFrame:\n",
    "    y_replacements = {\n",
    "        \"zs\": \"zs\",\n",
    "    } if not tot else {\n",
    "        \"zs\": \"sc-zs\",\n",
    "        \"spp\": \"sc-spp\",\n",
    "        \"cot\": \"sc-cot\",\n",
    "    }\n",
    "    x_replacements = {\n",
    "        \"zs\": \"zs\",\n",
    "    } \n",
    "\n",
    "    # {(model, prompt) -> {opponent_type -> total_points}}\n",
    "    round_data = defaultdict(lambda: defaultdict(float))\n",
    "    opponent_set = set()\n",
    "\n",
    "    for model in model_names:\n",
    "        for prompt in prompt_types:\n",
    "            for itr in range(iter_cnt):\n",
    "                directory = os.path.join(log_dir, f\"iteration_{itr}\", model, game_type, game_settings_type)\n",
    "\n",
    "                if not os.path.isdir(directory):\n",
    "                    continue\n",
    "\n",
    "                for game_dir in sorted(os.listdir(directory)):\n",
    "                    info_path = os.path.join(directory, game_dir, 'game.json')\n",
    "                    if not os.path.isfile(info_path):\n",
    "                        continue\n",
    "\n",
    "                    with open(info_path) as f:\n",
    "                        info = json.load(f)\n",
    "\n",
    "                    player_types = [info.get(f\"player_{i}_player_type\") for i in range(2)]\n",
    "                    if prompt not in player_types:\n",
    "                        continue\n",
    "\n",
    "                    model_idx = player_types.index(prompt)\n",
    "                    if model_idx != 0:\n",
    "                        continue\n",
    "\n",
    "                    opponent_type = player_types[1 - model_idx]\n",
    "                    opponent_set.add(opponent_type)\n",
    "\n",
    "                    tokens = info.get(f\"player_{model_idx}_tokens\")\n",
    "                    moves = info.get(f\"player_{model_idx}_moves\")\n",
    "                    op_moves = info.get(f\"player_{1 - model_idx}_moves\")\n",
    "\n",
    "                    if tokens is None or moves is None or op_moves is None:\n",
    "                        print(f\"Model {model}, Prompt {prompt}, Iteration {itr}, Game directory {game_dir} - Missing data in {info_path}\")\n",
    "                        raise ValueError(f\"Missing tokens or moves for {info_path}\")\n",
    "\n",
    "                    win_rates = []\n",
    "                    wins = 0\n",
    "                    for rounds_aux, (move, op_move) in enumerate(zip(reversed(moves), reversed(op_moves))):\n",
    "                        rounds = rounds_aux + 1  # Rounds are 1-indexed in the game\n",
    "                        flag = optimal_move(move, op_move, info[\"game_settings\"])\n",
    "                        if flag:\n",
    "                            wins += 1\n",
    "                        win_rate = wins / rounds\n",
    "                        win_rates.insert(0, win_rate)  # Insert at the beginning to keep order\n",
    "                    \n",
    "                    target_percentage = 0.9\n",
    "                    round_of_understood = next((i + 1 for i, rate in enumerate(win_rates) if rate >= target_percentage), len(win_rates) + 1)\n",
    "                    \n",
    "                    round_data[(model, prompt)][opponent_type] += round_of_understood\n",
    "\n",
    "\n",
    "    if not round_data:\n",
    "        raise ValueError(\"No data collected â€” check log paths and model+prompt naming conventions.\")\n",
    "\n",
    "    for key in round_data:\n",
    "        for opponent in round_data[key]:\n",
    "            round_data[key][opponent] /= iter_cnt\n",
    "\n",
    "    opponent_types_aux = [\"zs\", \"spp\", \"cot\", \"srep\", \"pp\", \"mf\", \"tft\"]\n",
    "    sorted_opponents = [opp for opp in opponent_types_aux if opp in opponent_set]\n",
    "    model_prompt_keys = [(model, prompt) for model in model_names for prompt in prompt_types]\n",
    "\n",
    "    # Apply x label replacements\n",
    "    x_labels = sorted_opponents.copy()\n",
    "    for old, new in x_replacements.items():\n",
    "        x_labels = [label.replace(old, new) for label in x_labels]\n",
    "\n",
    "    rows = []\n",
    "    index_tuples = []\n",
    "\n",
    "    for model, prompt in model_prompt_keys:\n",
    "        new_prompt = prompt\n",
    "        for old, new in y_replacements.items():\n",
    "            new_prompt = new_prompt.replace(old, new)\n",
    "\n",
    "        index_tuples.append( (model, new_prompt) )\n",
    "\n",
    "        values = []\n",
    "        for opp in sorted_opponents:\n",
    "            val = round_data.get((model, prompt), {}).get(opp, -1000)\n",
    "            values.append(val)\n",
    "        rows.append(values)\n",
    "\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(index_tuples, names=[\"model\", \"prompt\"])\n",
    "    df = pd.DataFrame(rows, index=index, columns=x_labels)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_nonsc_r = [\n",
    "    get_round_of_understood_opponent(\n",
    "        log_dir=\"../logs_pd/logs_3/data\",\n",
    "        model_names=[model[\"name\"] for model in models],\n",
    "        prompt_types=prompt_types,\n",
    "        game_type=\"pd\",\n",
    "        game_settings_type=game_settings_type,\n",
    "        iter_cnt=5,\n",
    "        tot=False,\n",
    "    )\n",
    "    for game_settings_type in game_settings_types\n",
    "]\n",
    "\n",
    "dfs_sc_r = [\n",
    "    get_round_of_understood_opponent(\n",
    "        log_dir=\"../logs_pd/logs_3/data_tot\",\n",
    "        model_names=[model[\"name\"] for model in models],\n",
    "        prompt_types=prompt_types,\n",
    "        game_type=\"pd\",\n",
    "        game_settings_type=game_settings_type,\n",
    "        iter_cnt=2,\n",
    "        tot=True,\n",
    "    )\n",
    "    for game_settings_type in game_settings_types\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_order = ['zs', 'cot', 'spp', 'sc-zs', 'sc-cot', 'sc-spp']\n",
    "prompt_order_map = {prompt: i for i, prompt in enumerate(prompt_order)}\n",
    "\n",
    "dfs_merged_r = []\n",
    "\n",
    "for df_nonsc, df_sc in zip(dfs_nonsc_r, dfs_sc_r):\n",
    "    df_nonsc = df_nonsc.copy()\n",
    "    df_sc = df_sc.copy()\n",
    "\n",
    "    # Prefix the prompt types in sc\n",
    "    new_index = []\n",
    "    for model, prompt in df_sc.index:\n",
    "        new_index.append((model, prompt))\n",
    "    df_sc.index = pd.MultiIndex.from_tuples(new_index, names=df_sc.index.names)\n",
    "\n",
    "    # Concatenate vertically\n",
    "    merged_df = pd.concat([df_nonsc, df_sc])\n",
    "\n",
    "    # Reorder by (model, prompt) with custom prompt order\n",
    "    merged_df = merged_df.reset_index()\n",
    "\n",
    "    # Add a sort key column\n",
    "    merged_df['prompt_order'] = merged_df['prompt'].map(prompt_order_map)\n",
    "\n",
    "    # Sort by model, then prompt order\n",
    "    merged_df = merged_df.sort_values(['model', 'prompt_order'])\n",
    "\n",
    "    # Drop helper column\n",
    "    merged_df = merged_df.drop(columns=['prompt_order'])\n",
    "\n",
    "    # Restore MultiIndex\n",
    "    merged_df = merged_df.set_index(['model', 'prompt'])\n",
    "\n",
    "    dfs_merged_r.append(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, game_settings_type in zip(dfs_merged_r, game_settings_types):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify maxima\n",
    "    row_max_mask = df.eq(df.min(axis=1), axis=0)\n",
    "    col_max_mask = df.eq(df.min(axis=0), axis=1)\n",
    "\n",
    "    # Make values strings, bold if min in row or column\n",
    "    styled_df = df.copy().astype(str)\n",
    "    for row in df.index:\n",
    "        for col in df.columns:\n",
    "            val = df.loc[row, col]\n",
    "            is_max = row_max_mask.loc[row, col] or col_max_mask.loc[row, col]\n",
    "            if pd.isna(val):\n",
    "                formatted = \"\"\n",
    "            else:\n",
    "                formatted = f\"\\\\textbf{{{val:.1f}}}\" if is_max else f\"{val:.1f}\"\n",
    "            styled_df.loc[row, col] = formatted\n",
    "\n",
    "    # Add game_settings_type as MultiIndex header\n",
    "    styled_df.columns = pd.MultiIndex.from_product(\n",
    "        [[game_settings_type], styled_df.columns],\n",
    "    )\n",
    "\n",
    "    # Output LaTeX\n",
    "    latex_code = styled_df.to_latex(\n",
    "        index=True,\n",
    "        multirow=True,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format='c',\n",
    "        escape=False,  # Allow \\textbf\n",
    "        caption=f\"Round \\\\# where the Agent understood the opponent's Strategy ({game_settings_type})\",\n",
    "        label=f\"tab:pd_round_heatmap_{game_settings_type}\",\n",
    "    )\n",
    "    print(latex_code)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the opponent you want to extract\n",
    "target_opponent = \"avg\"\n",
    "\n",
    "# Assume you have:\n",
    "# - dfs_merged: list of merged DataFrames (one per game_settings_type)\n",
    "# - game_settings_types: list of corresponding names\n",
    "\n",
    "# Collect slices for the target opponent from each df\n",
    "slices = []\n",
    "\n",
    "for df, setting in zip(dfs_merged_ef, game_settings_types):\n",
    "    df = df.copy()\n",
    "    if target_opponent not in df.columns:\n",
    "        raise ValueError(f\"Opponent '{target_opponent}' not found in columns of {setting}.\")\n",
    "    \n",
    "    # Series with (model, prompt) index and opponent points as values\n",
    "    s = df[target_opponent].rename(setting)\n",
    "    slices.append(s)\n",
    "\n",
    "# Merge all into one DataFrame on index\n",
    "result_df = pd.concat(slices, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_latex_heatmap(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify maxima\n",
    "    row_max_mask = df.eq(df.max(axis=1), axis=0)\n",
    "    col_max_mask = df.eq(df.max(axis=0), axis=1)\n",
    "\n",
    "    # Make values strings, bold if min in row or column\n",
    "    styled_df = df.copy().astype(str)\n",
    "    for row in df.index:\n",
    "        for col in df.columns:\n",
    "            val = df.loc[row, col]\n",
    "            is_max = row_max_mask.loc[row, col] or col_max_mask.loc[row, col]\n",
    "            if pd.isna(val):\n",
    "                formatted = \"\"\n",
    "            else:\n",
    "                formatted = f\"\\\\textbf{{{val:.2f}}}\" if is_max else f\"{val:.2f}\"\n",
    "            styled_df.loc[row, col] = formatted\n",
    "\n",
    "    # Output LaTeX\n",
    "    latex_code = styled_df.to_latex(\n",
    "        index=True,\n",
    "        multirow=True,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format='c',\n",
    "        escape=False,  # Allow \\textbf\n",
    "        caption=\"Average Efficiency (Points per kilo-token)\",\n",
    "        label=\"tab:pd_efficiency_avg_heatmap\",\n",
    "    )\n",
    "    print(latex_code)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print_latex_heatmap(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_valid_rate(\n",
    "    log_dir: str,\n",
    "    model_names: list[str],\n",
    "    prompt_types: list[str],\n",
    "    game_type: str,\n",
    "    game_settings_type: str,\n",
    "    iter_cnt: int,\n",
    "    tot: bool,\n",
    ") -> pd.DataFrame:\n",
    "    y_replacements = {\n",
    "        \"zs\": \"zs\",\n",
    "    } if not tot else {\n",
    "        \"zs\": \"sc-zs\",\n",
    "        \"spp\": \"sc-spp\",\n",
    "        \"cot\": \"sc-cot\",\n",
    "    }\n",
    "    x_replacements = {\n",
    "        \"zs\": \"zs\",\n",
    "    } \n",
    "\n",
    "    # {(model, prompt) -> {opponent_type -> total_points}}\n",
    "    valid_data = defaultdict(lambda: defaultdict(float))\n",
    "    opponent_set = set()\n",
    "\n",
    "    for model in model_names:\n",
    "        for prompt in prompt_types:\n",
    "            for itr in range(iter_cnt):\n",
    "                directory = os.path.join(log_dir, f\"iteration_{itr}\", model, game_type, game_settings_type)\n",
    "\n",
    "                if not os.path.isdir(directory):\n",
    "                    continue\n",
    "\n",
    "                for game_dir in sorted(os.listdir(directory)):\n",
    "                    info_path = os.path.join(directory, game_dir, 'game.json')\n",
    "                    if not os.path.isfile(info_path):\n",
    "                        continue\n",
    "\n",
    "                    with open(info_path) as f:\n",
    "                        info = json.load(f)\n",
    "\n",
    "                    player_types = [info.get(f\"player_{i}_player_type\") for i in range(2)]\n",
    "                    if prompt not in player_types:\n",
    "                        continue\n",
    "\n",
    "                    model_idx = player_types.index(prompt)\n",
    "                    if model_idx != 0:\n",
    "                        continue\n",
    "\n",
    "                    opponent_type = player_types[1 - model_idx]\n",
    "                    opponent_set.add(opponent_type)\n",
    "\n",
    "                    valid_outcomes = info.get(\"valid_outcomes\")\n",
    "\n",
    "                    if valid_outcomes is None:\n",
    "                        print(f\"Model {model}, Prompt {prompt}, Iteration {itr}, Game directory {game_dir} - Missing data in {info_path}\")\n",
    "                        raise ValueError(f\"Missing moves for {info_path}\")\n",
    "\n",
    "                    # find percentage of true valid outcomes\n",
    "                    total_outcomes = len(valid_outcomes)\n",
    "                    if total_outcomes == 0:\n",
    "                        print(f\"Model {model}, Prompt {prompt}, Iteration {itr}, Game directory {game_dir} - No valid outcomes in {info_path}\")\n",
    "                        continue\n",
    "\n",
    "                    valid_count = sum(1 for outcome in valid_outcomes if outcome)\n",
    "                    valid_rate = valid_count / total_outcomes\n",
    "                    \n",
    "                    valid_data[(model, prompt)][opponent_type] += valid_rate\n",
    "\n",
    "\n",
    "    if not valid_data:\n",
    "        raise ValueError(\"No data collected â€” check log paths and model+prompt naming conventions.\")\n",
    "\n",
    "    for key in valid_data:\n",
    "        for opponent in valid_data[key]:\n",
    "            valid_data[key][opponent] /= iter_cnt\n",
    "            valid_data[key][opponent] *= 100  # Convert to percentage\n",
    "\n",
    "    opponent_types_aux = [\"zs\", \"spp\", \"cot\", \"srep\", \"pp\", \"mf\", \"tft\"]\n",
    "    sorted_opponents = [opp for opp in opponent_types_aux if opp in opponent_set]\n",
    "    model_prompt_keys = [(model, prompt) for model in model_names for prompt in prompt_types]\n",
    "\n",
    "    # Apply x label replacements\n",
    "    x_labels = sorted_opponents.copy()\n",
    "    for old, new in x_replacements.items():\n",
    "        x_labels = [label.replace(old, new) for label in x_labels]\n",
    "\n",
    "    rows = []\n",
    "    index_tuples = []\n",
    "\n",
    "    for model, prompt in model_prompt_keys:\n",
    "        new_prompt = prompt\n",
    "        for old, new in y_replacements.items():\n",
    "            new_prompt = new_prompt.replace(old, new)\n",
    "\n",
    "        index_tuples.append( (model, new_prompt) )\n",
    "\n",
    "        values = []\n",
    "        for opp in sorted_opponents:\n",
    "            val = valid_data.get((model, prompt), {}).get(opp, -1000)\n",
    "            values.append(val)\n",
    "        rows.append(values)\n",
    "\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(index_tuples, names=[\"model\", \"prompt\"])\n",
    "    df = pd.DataFrame(rows, index=index, columns=x_labels)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_nonsc_v = [\n",
    "    get_valid_rate(\n",
    "        log_dir=\"../logs_pd/logs_3/data\",\n",
    "        model_names=[model[\"name\"] for model in models],\n",
    "        prompt_types=prompt_types,\n",
    "        game_type=\"pd\",\n",
    "        game_settings_type=game_settings_type,\n",
    "        iter_cnt=5,\n",
    "        tot=False,\n",
    "    )\n",
    "    for game_settings_type in game_settings_types\n",
    "]\n",
    "\n",
    "dfs_sc_v = [\n",
    "    get_valid_rate(\n",
    "        log_dir=\"../logs_pd/logs_3/data_tot\",\n",
    "        model_names=[model[\"name\"] for model in models],\n",
    "        prompt_types=prompt_types,\n",
    "        game_type=\"pd\",\n",
    "        game_settings_type=game_settings_type,\n",
    "        iter_cnt=2,\n",
    "        tot=True,\n",
    "    )\n",
    "    for game_settings_type in game_settings_types\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_order = ['zs', 'cot', 'spp', 'sc-zs', 'sc-cot', 'sc-spp']\n",
    "prompt_order_map = {prompt: i for i, prompt in enumerate(prompt_order)}\n",
    "\n",
    "dfs_merged_v = []\n",
    "\n",
    "for df_nonsc, df_sc in zip(dfs_nonsc_v, dfs_sc_v):\n",
    "    df_nonsc = df_nonsc.copy()\n",
    "    df_sc = df_sc.copy()\n",
    "\n",
    "    # Prefix the prompt types in sc\n",
    "    new_index = []\n",
    "    for model, prompt in df_sc.index:\n",
    "        new_index.append((model, prompt))\n",
    "    df_sc.index = pd.MultiIndex.from_tuples(new_index, names=df_sc.index.names)\n",
    "\n",
    "    # Concatenate vertically\n",
    "    merged_df = pd.concat([df_nonsc, df_sc])\n",
    "\n",
    "    # Reorder by (model, prompt) with custom prompt order\n",
    "    merged_df = merged_df.reset_index()\n",
    "\n",
    "    # Add a sort key column\n",
    "    merged_df['prompt_order'] = merged_df['prompt'].map(prompt_order_map)\n",
    "\n",
    "    # Sort by model, then prompt order\n",
    "    merged_df = merged_df.sort_values(['model', 'prompt_order'])\n",
    "\n",
    "    # Drop helper column\n",
    "    merged_df = merged_df.drop(columns=['prompt_order'])\n",
    "\n",
    "    # Restore MultiIndex\n",
    "    merged_df = merged_df.set_index(['model', 'prompt'])\n",
    "\n",
    "    dfs_merged_v.append(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_averages = {}\n",
    "\n",
    "for df, game_setting in zip(dfs_merged_v, game_settings_types):\n",
    "    # df has index (model, prompt), and columns = opponent types\n",
    "\n",
    "    # Step 1: average across opponents\n",
    "    df_mean_opponents = df.mean(axis=1)  # now index is (model, prompt), values = mean vs opponents\n",
    "\n",
    "    # Step 2: group by model and average across prompt styles\n",
    "    avg_by_model = df_mean_opponents.groupby(level=0).mean()  # model -> average\n",
    "\n",
    "    model_averages[game_setting] = avg_by_model\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "df_model_avg = pd.DataFrame(model_averages)\n",
    "\n",
    "# average across game settings\n",
    "df_model_avg['avg'] = df_model_avg.mean(axis=1)\n",
    "\n",
    "# I only want the average column\n",
    "df_model_avg = df_model_avg[['avg']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Average Valid Rate (\\% of Valid Outcomes)}\n",
      "\\label{tab:pd_valid_rates}\n",
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      " & avg \\\\\n",
      "model &  \\\\\n",
      "\\midrule\n",
      "Claude 3.5 Sonnet v2 & 100.00 \\\\\n",
      "Claude 3.7 Sonnet & 100.00 \\\\\n",
      "Claude 3.7 Sonnet (Thinking) & 100.00 \\\\\n",
      "Claude Sonnet 4 & 100.00 \\\\\n",
      "Claude Sonnet 4 (Thinking) & 100.00 \\\\\n",
      "DeepSeek-R1 & 99.23 \\\\\n",
      "Llama 3.3 70B Instruct & 100.00 \\\\\n",
      "Mistral Large (24.07) & 99.45 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_valid_rates(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Make values strings, bold if min in row or column\n",
    "    styled_df = df.copy().astype(str)\n",
    "    for row in df.index:\n",
    "        for col in df.columns:\n",
    "            val = df.loc[row, col]\n",
    "            is_max = False\n",
    "            if pd.isna(val):\n",
    "                formatted = \"\"\n",
    "            else:\n",
    "                formatted = f\"\\\\textbf{{{val:.2f}}}\" if is_max else f\"{val:.2f}\"\n",
    "            styled_df.loc[row, col] = formatted\n",
    "\n",
    "    # Output LaTeX\n",
    "    latex_code = styled_df.to_latex(\n",
    "        index=True,\n",
    "        multirow=True,\n",
    "        multicolumn=True,\n",
    "        multicolumn_format='c',\n",
    "        escape=False,  # Allow \\textbf\n",
    "        caption=\"Average Valid Rate (\\\\% of Valid Outcomes)\",\n",
    "        label=\"tab:pd_valid_rates\",\n",
    "    )\n",
    "    print(latex_code)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print_valid_rates(df_model_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
